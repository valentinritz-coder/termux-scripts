dummycd ~/src/llama.cpp
./build/bin/llama-server \
  -m /sdcard/cfl_watch/models/qwen2.5-0.5b-instruct-q4_k_m.gguf \
  --host 127.0.0.1 --port 8001 \
  -c 2048 -t 4 \
  --alias local-model


----

export LLM_INSTRUCTION="Luxembourg -> Arlon, train only, now"
export OPENAI_BASE_URL="http://127.0.0.1:8001"
export OPENAI_API_KEY="dummy"   # ou ton token si besoin
export LLM_MODEL="local-model"
bash ~/cfl_watch/runner.sh --scenario scenario_llm_tripplanner.sh
